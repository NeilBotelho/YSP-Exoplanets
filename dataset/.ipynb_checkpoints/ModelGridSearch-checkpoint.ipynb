{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import math as m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I read the simple Imputed mice cart dataset into data and store the habitable exoplanets ids in habitableRows\n",
    "* i remove the id and habitable columns and standardise th data between one and zero.\n",
    "* i use PCA to keeo only 98% of the data variance and\n",
    "* hence decrease dimensionality by about half\n",
    "* I then rejoin the habitable and id columns and store the result in preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('simpleImputedMiceCart')\n",
    "habitableRows=[151, 152, 153, 1604, 2155, 2223, 2882, 3133, 3606, 3716, 3742, 3744]\n",
    "exTest= 3743\n",
    "habitable=data.row_id.isin(habitableRows).replace(True,1).rename('habitable')\n",
    "#Store row ids and remove for scaling and then PCA\n",
    "row_id=data.row_id\n",
    "data=data.drop('row_id',axis=1)\n",
    "scaledData=pd.DataFrame(StandardScaler().fit_transform(data),columns=data.columns)\n",
    "pca = PCA().fit(scaledData)\n",
    "\n",
    "#KEEP 98% of variance get rid of other components\n",
    "numComponents=0\n",
    "for n in np.cumsum(pca.explained_variance_ratio_):\n",
    "    if n<0.980:\n",
    "        numComponents+=1\n",
    "pca=PCA(n_components=numComponents).fit_transform(scaledData)\n",
    "print('working')\n",
    "\n",
    "#Get row id back and shuffle\n",
    "preprocessed=pd.concat([row_id,pd.DataFrame(pca),habitable],axis=1)\n",
    "exTest=preprocessed[preprocessed.row_id==exTest].drop('row_id',axis=1)\n",
    "preprocessed=preprocessed[preprocessed.row_id!=3743]\n",
    "preprocessed=shuffle(preprocessed,random_state=100).reset_index()\n",
    "preprocessed=preprocessed.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the processed dataset into non habitable and habitable so that we get a 20% habitable in the final test set\n",
    "habitableExoplanets=preprocessed[preprocessed.row_id.isin(habitableRows)]\n",
    "nonHabitableExoplanets=preprocessed[~preprocessed.row_id.isin(habitableRows)]\n",
    "trainCols=[x for x in habitableExoplanets.columns if x not in ['habitable','row_id']]\n",
    "\n",
    "#habitable planets train test split\n",
    "hTrainX,hTestX,hTrainY,hTestY=train_test_split(habitableExoplanets[trainCols],habitableExoplanets['habitable'])\n",
    "\n",
    "#nonhabitable planet train test split\n",
    "trainX, testX,trainY,testY=train_test_split(nonHabitableExoplanets[trainCols],nonHabitableExoplanets['habitable'])\n",
    "\n",
    "#joining the 2 splits\n",
    "trainX=pd.concat([trainX,hTrainX])\n",
    "trainY=pd.concat([trainY,hTrainY])\n",
    "testX=pd.concat([testX,hTestX])\n",
    "testY=pd.concat([testY,hTestY])\n",
    "\n",
    "# return trainX,testX,trainY,testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 fold generator\n",
    "def cvGen(X,Y):\n",
    "#     X=preprocessed[xcols]\n",
    "#     Y=preprocessed[['habitable','row_id']]\n",
    "    out=[]\n",
    "    nFolds=5\n",
    "    x=X[~X.row_id.isin(habitableRows)]\n",
    "    y=Y[~Y.row_id.isin(habitableRows)]\n",
    "    stepSize=round(len(x)/nFolds)\n",
    "    low=0\n",
    "    h=habitableRows\n",
    "    cutSize=m.floor(len(h)/nFolds)\n",
    "    for n in range(nFolds-1):\n",
    "        train=pd.concat([x.iloc[low:low+stepSize,],X[X.row_id.isin(h[0:cutSize])]]).drop(\"row_id\",axis=1)\n",
    "        target=pd.concat([y.iloc[low:low+stepSize,],Y[Y.row_id.isin(h[0:cutSize])]]).drop(\"row_id\",axis=1)\n",
    "#         yield (train,target)\n",
    "        out.append((train.index.values.astype(int),target.index.values.astype(int)))\n",
    "        h=h[cutSize:]\n",
    "        low=low+stepSize\n",
    "\n",
    "    train=pd.concat([x.iloc[low:,],X[X.row_id.isin(h[0:])]])\n",
    "    target=pd.concat([y.iloc[low:,],Y[Y.row_id.isin(h[0:])]])\n",
    "    out.append((train.index.values.astype(int),target.index.values.astype(int)))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Example Cell for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "penalty = ['l1', 'l2']\n",
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
    "solver = ['liblinear', 'saga']\n",
    "\n",
    "param_grid = dict(penalty=penalty,\n",
    "                  C=C,\n",
    "                  class_weight=class_weight,\n",
    "                  solver=solver)\n",
    "logistic=LogisticRegression()\n",
    "grid = GridSearchCV(estimator=logistic,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='roc_auc',\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"4\">SGD Classifier</font>\n",
    "* <font size=\"4\">Perceptron</font>\n",
    "* <font size=\"4\">Random Forests Classifier</font>\n",
    "* <font size=\"4\">XGBoost</font>\n",
    "* <font size=\"4\">Gaussian Process Classification</font>\n",
    "* <font size=\"4\">Naive Bayes</font>\n",
    "* <font size=\"4\">K neighbours</font>\n",
    "* <font size=\"4\">SVC</font>\n",
    "* <font size=\"4\">ensemble classifiers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neil/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 out of 192 | elapsed:   33.5s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "kernel=['linear', 'poly', 'rbf', 'sigmoid', 'precomputed' ]\n",
    "degree=[1,2,3,4]\n",
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gamma=['auto','scale']\n",
    "param_grid = dict(C=C,\n",
    "                  gamma=gamma,\n",
    "#                   kernel=kernel,\n",
    "                  degree=degree)\n",
    "xcols=[x for x in preprocessed.columns if x not in ['habitable']]\n",
    "cvIter=cvGen(preprocessed[xcols],preprocessed[['habitable','row_id']])\n",
    "SVC_GridSearch=GridSearchCV(SVC(),param_grid,scoring='roc_auc',verbose=1,n_jobs=-1)\n",
    "print(\"running\")\n",
    "\n",
    "SgridSearch=SVC_GridSearch.fit(preprocessed[xcols],preprocessed.habitable.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'degree': 1, 'gamma': 'scale'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_GridSearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996042216358839"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "test=SVC(C=0.01,degree=1,gamma='scale')\n",
    "test.fit(trainX,trainY)\n",
    "test.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
